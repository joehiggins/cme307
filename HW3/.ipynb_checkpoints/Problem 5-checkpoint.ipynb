{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group: Austin Wang, Joe Higgins, Lawrence Moore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "e = np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent our data from being linearly separable (and therefore leading the solution blowing up for logistic regression), we slightly alter the (0,0) data points so that the data is no longer linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_data = np.matrix([\n",
    "    [ -0.01, -0.01],\n",
    "    [ 1, 0],\n",
    "    [ 0, 1]\n",
    "])\n",
    "neg_data = np.matrix([\n",
    "    [ 0.01, 0.01],\n",
    "    [-1, 0],\n",
    "    [ 0,-1]\n",
    "])\n",
    "\n",
    "# Add shift variable\n",
    "pos_data = np.hstack((pos_data, np.ones((3,1))))\n",
    "neg_data = np.hstack((neg_data, np.ones((3,1))))\n",
    "\n",
    "# Define gradient function\n",
    "def grad(x):\n",
    "    \n",
    "    pos_data_sum = \\\n",
    "        e**(-1*np.dot(pos_data[0,:], x))/(1 + e**(-1*np.dot(pos_data[0,:], x))) * \\\n",
    "        pos_data[0,:] + e**(-1*np.dot(pos_data[1,:], x))/ \\\n",
    "        (1 + e**(-1*np.dot(pos_data[1,:], x))) * pos_data[1,:] + \\\n",
    "        e**(-1*np.dot(pos_data[2,:], x))/(1 + e**(-1*np.dot(pos_data[2,:], x))) * \\\n",
    "        pos_data[2,:]\n",
    "\n",
    "    neg_data_sum = \\\n",
    "        e**(np.dot(neg_data[0,:], x))/(1 + e**(np.dot(neg_data[0,:], x))) * \\\n",
    "        neg_data[0,:] + e**(np.dot(neg_data[1,:], x))/ \\\n",
    "        (1 + e**(np.dot(neg_data[1,:], x))) * neg_data[1,:] + \\\n",
    "        e**(np.dot(neg_data[2,:], x))/(1 + e**(np.dot(neg_data[2,:], x))) * \\\n",
    "        neg_data[2,:]\n",
    "        \n",
    "    return np.transpose(-1*pos_data_sum + neg_data_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize looping variables\n",
    "x_0 = np.matrix([\n",
    "    [0], \n",
    "    [0],\n",
    "    [0]\n",
    "])\n",
    "check = 1000\n",
    "max_iter = 10000\n",
    "k = 0\n",
    "x_k = x_0\n",
    "\n",
    "# Perform algorithm\n",
    "alpha = 0.5 # Learning rate\n",
    "while check > 10**-8 and k < max_iter:\n",
    "\n",
    "    x_k1 = x_k - alpha * grad(x_k)\n",
    "    check = np.linalg.norm(x_k1 - x_k)\n",
    "    x_k = x_k1\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution converged to: \n",
      "[[5.24199363]\n",
      " [5.24199363]\n",
      " [0.        ]]\n",
      "Number of Iterations Necessary:  2557\n"
     ]
    }
   ],
   "source": [
    "print('Solution converged to: ')\n",
    "print(x_k1)\n",
    "print('Number of Iterations Necessary: ', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accelerated Steepest Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get next lambda\n",
    "def next_lam(x):\n",
    "    res = (1+np.sqrt(1+4*(x)**2))/2\n",
    "    return res\n",
    "\n",
    "# Initial values\n",
    "x_k = x_0\n",
    "x_k_til = np.matrix((np.random.rand(3,1)*0.01))\n",
    "lam_k = 0\n",
    "check = 1000\n",
    "max_iter = 10000\n",
    "k = 0\n",
    "beta = 1\n",
    "\n",
    "# Perform algorithm\n",
    "while check > 10**-8 and k < max_iter:\n",
    "    \n",
    "    x_k_til1 = x_k-(1/beta)*grad(x_k)\n",
    "    lam_k1 = next_lam(lam_k)\n",
    "    alpha_k = (1-lam_k)/lam_k1\n",
    "    x_k1 = (1-alpha_k)*x_k_til1 + alpha_k*x_k_til\n",
    "    check = np.linalg.norm(x_k1 - x_k)\n",
    "    x_k = x_k1\n",
    "    x_k_til = x_k_til1\n",
    "    lam_k = lam_k1\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution converged to: \n",
      "[[ 5.24195789e+00]\n",
      " [ 5.24195789e+00]\n",
      " [-1.76615718e-16]]\n",
      "Number of Iterations Necessary:  805\n"
     ]
    }
   ],
   "source": [
    "print('Solution converged to: ')\n",
    "print(x_k1)\n",
    "print('Number of Iterations Necessary: ', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial values\n",
    "check = 1000\n",
    "max_iter = 10000\n",
    "k = 0\n",
    "x_k = x_0\n",
    "g_0 = grad(x_0)\n",
    "g_k = g_0\n",
    "d_0 = -1 * g_0\n",
    "d_k = d_0\n",
    "alpha = 0.2\n",
    "\n",
    "# Perform algorithm\n",
    "while check > 10**-8 and k < max_iter:\n",
    "\n",
    "    # Next iterates\n",
    "    x_k1 = x_k + alpha * d_k\n",
    "    g_k1 = grad(x_k1)\n",
    "    B_k = (np.linalg.norm(g_k1)**2)/(np.linalg.norm(g_k)**2)\n",
    "    d_k1 = -1*g_k1 + B_k*d_k\n",
    "    \n",
    "    # Set next iterates as current iterates\n",
    "    check = np.linalg.norm(x_k1 - x_k)\n",
    "    x_k = x_k1\n",
    "    d_k = d_k1\n",
    "    g_k = g_k1\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution converged to: \n",
      "[[5.24199482]\n",
      " [5.24199482]\n",
      " [0.        ]]\n",
      "Number of Iterations Necessary:  371\n"
     ]
    }
   ],
   "source": [
    "print('Solution converged to: ')\n",
    "print(x_k1)\n",
    "print('Number of Iterations Necessary: ', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BB Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get new alpha\n",
    "def new_alpha(x_k, x_km1):\n",
    "    delta_x = np.subtract(x_k, x_km1)\n",
    "    delta_grad_x = np.subtract(grad(x_k), grad(x_km1))\n",
    "    return np.asscalar(np.dot(np.transpose(delta_x), delta_grad_x) / \n",
    "                       np.dot(np.transpose(delta_grad_x), delta_grad_x))\n",
    "\n",
    "# Initial values\n",
    "max_iter = 10000\n",
    "k = 0\n",
    "alpha = 0.5\n",
    "x_km1 = x_0\n",
    "x_k = x_km1 - alpha * grad(x_km1)\n",
    "check = np.linalg.norm(x_k - x_km1)\n",
    "\n",
    "# Peform algorithm\n",
    "while check > 10**-8 and k < max_iter:\n",
    "    alpha = new_alpha(x_k, x_km1)\n",
    "    x_kp1 = x_k - alpha * grad(x_k)\n",
    "    check = np.linalg.norm(x_kp1 - x_k)\n",
    "\n",
    "    x_km1 = x_k\n",
    "    x_k = x_kp1\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution converged to: \n",
      "[[5.24199496]\n",
      " [5.24199496]\n",
      " [0.        ]]\n",
      "Number of Iterations Necessary:  12\n"
     ]
    }
   ],
   "source": [
    "print('Solution converged to: ')\n",
    "print(x_k)\n",
    "print('Number of Iterations Necessary: ', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using a tolerance of 10^-8, and with the data slightly altered to make it linearly separable, we found that the algorithms from fastest to slowest are:\n",
    "\n",
    "1. BB Method (12 iterations)\n",
    "2. Conjugate Direction (371 iterations)\n",
    "3. Accelerated Steepest Descent (805 iterations)\n",
    "4. Steepest Descent (2557 iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
