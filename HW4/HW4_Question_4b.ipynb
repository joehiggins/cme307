{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group: Austin Wang, Joe Higgins, Lawrence Moore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "e = np.exp(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "As we did in Homework 3, we slightly alter the data points so that the data is no longer linearly separable. For the particular dataset from Homework 2 and Homework 3, this means subtracting 0.01 from both coordinates of the positive class (0,0) point, and adding 0.01 to both coordinates of the negative class (0,0) point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_data = np.matrix([\n",
    "    [ -0.01, -0.01],\n",
    "    [ 1, 0],\n",
    "    [ 0, 1]\n",
    "])\n",
    "neg_data = np.matrix([\n",
    "    [ 0.01, 0.01],\n",
    "    [-1, 0],\n",
    "    [ 0,-1]\n",
    "])\n",
    "\n",
    "# Add shift variable\n",
    "pos_data = np.hstack((pos_data, np.ones((3,1))))\n",
    "neg_data = np.hstack((neg_data, np.ones((3,1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Below are the implementations of the log-logistic-loss function gradient and Hessian. The gradient will be used in both the Quasi-Newton and Newton methods, while the Hessian will be used in only the Newton method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Gradient of the log-logistic-loss function\n",
    "def grad(x):\n",
    "    \n",
    "    pos_data_sum = \\\n",
    "        e**(-1*np.dot(pos_data[0,:], x))/(1 + e**(-1*np.dot(pos_data[0,:], x))) * \\\n",
    "        pos_data[0,:] + e**(-1*np.dot(pos_data[1,:], x))/ \\\n",
    "        (1 + e**(-1*np.dot(pos_data[1,:], x))) * pos_data[1,:] + \\\n",
    "        e**(-1*np.dot(pos_data[2,:], x))/(1 + e**(-1*np.dot(pos_data[2,:], x))) * \\\n",
    "        pos_data[2,:]\n",
    "\n",
    "    neg_data_sum = \\\n",
    "        e**(np.dot(neg_data[0,:], x))/(1 + e**(np.dot(neg_data[0,:], x))) * \\\n",
    "        neg_data[0,:] + e**(np.dot(neg_data[1,:], x))/ \\\n",
    "        (1 + e**(np.dot(neg_data[1,:], x))) * neg_data[1,:] + \\\n",
    "        e**(np.dot(neg_data[2,:], x))/(1 + e**(np.dot(neg_data[2,:], x))) * \\\n",
    "        neg_data[2,:]\n",
    "        \n",
    "    return np.transpose(-1*pos_data_sum + neg_data_sum)\n",
    "\n",
    "# Hessian of the log-logistic-loss function\n",
    "def hess(x):\n",
    "    output_hess = np.zeros((3, 3))\n",
    "    for j in range(3):\n",
    "        pos_sum = np.zeros((1,3))\n",
    "        neg_sum = np.zeros((1,3))\n",
    "        for i in range(3):\n",
    "            num = e**(-1 * np.dot(pos_data[i, :], x)) \\\n",
    "                  * pos_data[i, j]\n",
    "            den = (1 + e**(-1 * np.dot(pos_data[i, :], x)))**2\n",
    "            pos_sum += (num/den)*pos_data[i, :]\n",
    "                          \n",
    "            num = e**(np.dot(neg_data[i, :], x)) \\\n",
    "                  * neg_data[i, j]\n",
    "            den = (1 + e**(np.dot(neg_data[i, :], x)))**2\n",
    "            neg_sum += (num/den)*neg_data[i, :]\n",
    "        output_hess[j,:] = pos_sum + neg_sum\n",
    "    return output_hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi-Newton Method\n",
    "\n",
    "Below we implement the BFGS Quasi-Newton method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize algorithm variables\n",
    "alpha_k = 0.5 # Learning rate\n",
    "x_0 = np.matrix([\n",
    "    [0], \n",
    "    [0],\n",
    "    [0]\n",
    "])\n",
    "check = 1000\n",
    "max_iter = 10000\n",
    "k = 0\n",
    "x_k1 = x_0\n",
    "B_k = np.eye(3, 3)\n",
    "\n",
    "# Perform BFGS algorithm\n",
    "while check > 10**-8 and k < max_iter:\n",
    "    k += 1\n",
    "    x_k = x_k1\n",
    "    p_k = np.linalg.solve(B_k, -1 * grad(x_k))\n",
    "    s_k = alpha_k * p_k\n",
    "    x_k1 = x_k + s_k\n",
    "    alpha_k = alpha_k\n",
    "    y_k = grad(x_k1) - grad(x_k)\n",
    "    \n",
    "    middle = np.dot(y_k, np.transpose(y_k)) / \\\n",
    "        np.dot(np.transpose(y_k), s_k)\n",
    "    BS = np.dot(B_k, s_k)\n",
    "    last = np.dot(BS, np.transpose(BS)) / \\\n",
    "            np.dot(np.transpose(s_k), BS) \n",
    "    \n",
    "    B_k = B_k + middle - last\n",
    "    check = np.linalg.norm(grad(x_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution converged to: \n",
      "[[5.24199464]\n",
      " [5.24199464]\n",
      " [0.        ]]\n",
      "Number of Iterations Necessary:  33\n"
     ]
    }
   ],
   "source": [
    "print('Solution converged to: ')\n",
    "print(x_k1)\n",
    "print('Number of Iterations Necessary: ', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton Method\n",
    "\n",
    "Our implementation of the Newton method, which *does* use the Hessian is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_0 = np.matrix([\n",
    "    [0], \n",
    "    [0],\n",
    "    [0]\n",
    "])\n",
    "check = 1000\n",
    "max_iter = 10000\n",
    "k = 0\n",
    "x_k = x_0\n",
    "\n",
    "while check > 10**-8 and k < max_iter:\n",
    "\n",
    "    x_k1 = x_k - np.linalg.inv(hess(x_k)) * grad(x_k)\n",
    "    check = np.linalg.norm(x_k1 - x_k)\n",
    "    x_k = x_k1\n",
    "    k = k+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution converged to: \n",
      "[[5.24199496]\n",
      " [5.24199496]\n",
      " [0.        ]]\n",
      "Number of Iterations Necessary:  9\n"
     ]
    }
   ],
   "source": [
    "print('Solution converged to: ')\n",
    "print(x_k1)\n",
    "print('Number of Iterations Necessary: ', k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In the experiments above, we found that the Newton method convergence (9 iterations) was significantly faster than the Quasi-Newton method convergence (33 iterations). This is not surprising, as intuitively we are using more information about the objective function in the Newton method by providing the entire Hessian. When compared to the methods used in Homework 3, we find that the algorithms from fastest to slowest are:\n",
    "\n",
    "1. Newton Method (9 iterations)\n",
    "2. BB Method (12 iterations)\n",
    "3. Quasi-Newton BFGS Method (33 iterations)\n",
    "4. Conjugate Direction (371 iterations)\n",
    "5. Accelerated Steepest Descent (805 iterations)\n",
    "6. Steepest Descent (2557 iterations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
